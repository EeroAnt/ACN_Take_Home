{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7925620a",
   "metadata": {},
   "source": [
    "# Take-home Assignment by Eero Antikainen\n",
    "\n",
    "To start with, we can just run the script as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58af8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023348e",
   "metadata": {},
   "source": [
    "## Part 1: ETL\n",
    "\n",
    "At first I glanced at the csv's to see what's in them.\n",
    "\n",
    "customers.csv looked mostly ok, transactions.csv had a good amount of missing values.\n",
    "\n",
    "For this amount of data and the complexity of processes I planned to do, I decided SQLite is more than fine.\n",
    "\n",
    "To begin with, I thought that I'd load the data pretty much as is to the database for easy querying.\n",
    "\n",
    "So after setting up the database, the only transformations I decided to do here is to turn missing values into None instead of empty strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b837af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_customers(raw_data: list[tuple]) -> list[tuple]:\n",
    "  return [\n",
    "    (\n",
    "      int(customer_id) if customer_id else None,\n",
    "      country if country else None,\n",
    "      signup_date if signup_date else None,\n",
    "      email if email else None\n",
    "    )\n",
    "    for customer_id, country, signup_date, email in raw_data\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afebf290",
   "metadata": {},
   "source": [
    "This transformation allowed me to insert the missing values as NULL easily.\n",
    "\n",
    "## Part 2: Feature engineering\n",
    "\n",
    "I proceeded to do some anomaly checks to understand the problems of the data better.\n",
    "\n",
    "Results displayed from the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b418e7e",
   "metadata": {},
   "source": [
    "```2026-02-05 21:22:17,924 - INFO - Found 30 NULL values in transactions table column customer_id\n",
    "2026-02-05 21:22:17,924 - INFO - Total value: 1410.28 EUR\n",
    "2026-02-05 21:22:17,925 - INFO - Found 24644 NULL values in transactions table column currency\n",
    "2026-02-05 21:22:17,925 - INFO - Found 20220 NULL values in transactions table column category\n",
    "2026-02-05 21:22:17,929 - INFO - Total value: 887961.98 EUR\n",
    "2026-02-05 21:22:18,014 - INFO - 85435 transactions found where timestamp precedes users signup\n",
    "2026-02-05 21:22:18,033 - INFO - Total value: 3717602.34 EUR\n",
    "2026-02-05 21:22:18,083 - INFO - Found 2000 duplicated transaction ids\n",
    "2026-02-05 21:22:18,084 - INFO - Getting deeper comparison\n",
    "2026-02-05 21:22:18,213 - INFO - Found 2000 duplicated transactions\n",
    "2026-02-05 21:22:18,214 - INFO - All the duplicates are exact matches and can be filtered via 'GROUP BY transaction_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc00c8f",
   "metadata": {},
   "source": [
    "The first part was straightforward query of NULL values for each column in both tables and logging results, when found.\n",
    "\n",
    "I also logged the total values transferred to Euros (except for NULL values in currency column)\n",
    "\n",
    "Turns out the customers data was fine and transactions had more inconsistencies.\n",
    "\n",
    "I decided to assume that category was not required information, but others I would flag for closer inspection.\n",
    "\n",
    "Both tables had time related column in them so I wanted to check for inconsistencies there.\n",
    "\n",
    "And I found 3.7 million Eur worth of transactions by users before their signup. More flags.\n",
    "\n",
    "The last anomaly check was found by accident when I was looking at `average_days_between_transactions` (now `purchase_interval`) data and at the top of the list there was a user with 2 transactions, 0 days in between, and identical sum. This inspired me to check possible duplicate id's, after which I did a deeper comparison. I suppose one could've gone just with the deeper comparison straight away, but I thought there might have been transactions that share only the transaction id because of a race condition or something similar.\n",
    "\n",
    "With all this insight, I cleaned the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "from utils.logging_config import logger\n",
    "\n",
    "\n",
    "def clean_transactions(conn: sqlite3.Connection) -> None:\n",
    "  logger.info(\"Cleaning the dataset after anomaly analysis\")\n",
    "  cur = conn.cursor()\n",
    "  cur.execute(\n",
    "    \"\"\"\n",
    "      CREATE VIEW clean_transactions AS\n",
    "      SELECT\n",
    "        transaction_id,\n",
    "        transactions.customer_id customer_id,\n",
    "        amount,\n",
    "        currency,\n",
    "        timestamp,\n",
    "        category\n",
    "      FROM transactions\n",
    "      INNER JOIN customers\n",
    "      ON transactions.customer_id = customers.customer_id\n",
    "      WHERE currency IS NOT NULL\n",
    "        AND transactions.customer_id IS NOT NULL\n",
    "        AND transactions.timestamp >= customers.signup_date\n",
    "      GROUP BY transaction_id\n",
    "    \"\"\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb18a92",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "During the process I rotated the data a handful of ways to find something interesting. Here are some insights from the views I ended up in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64458d4",
   "metadata": {},
   "source": [
    "#### Customers with the lowest purchase interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936fabe",
   "metadata": {},
   "source": [
    "```Showing top 10 of purchase_interval ascending\n",
    "customer_id  transactions  avg_spent  max_spent  min_spent  total_spent    first_transaction     last_transaction  purchase_interval\n",
    "       1714             2     121.14     179.30      62.97       242.27  2020-11-18 23:04:00  2020-11-19 13:32:00                0.6\n",
    "       3980             2      97.74     118.38      77.09       195.47  2020-12-09 04:53:00  2020-12-10 05:43:00                1.0\n",
    "       2327             4     130.17     184.07      56.93       520.66  2020-11-10 08:58:00  2020-11-14 01:39:00                1.2\n",
    "       4391             3     119.97     183.59      87.04       359.92  2020-12-08 07:19:00  2020-12-11 21:24:00                1.8\n",
    "       3014             2     101.25     109.01      93.48       202.49  2020-11-09 15:09:00  2020-11-11 11:24:00                1.8\n",
    "        136             2     115.91     130.87     100.95       231.82  2020-11-05 15:54:00  2020-11-08 04:02:00                2.5\n",
    "       3637             2     110.53     113.18     107.87       221.05  2020-10-20 10:16:00  2020-10-23 12:06:00                3.1\n",
    "        117             2     169.10     187.12     151.07       338.19  2020-11-26 03:28:00  2020-11-29 13:05:00                3.4\n",
    "       3587             3     126.94     204.91      77.08       380.83  2020-09-16 13:25:00  2020-09-23 20:25:00                3.6\n",
    "       4280             5     139.75     197.68     104.88       698.75  2020-10-18 08:45:00  2020-11-02 17:48:00                3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142534f3",
   "metadata": {},
   "source": [
    "Here we see the lowest purchase intervals, the highest transaction frequencies. It seems that none of them have many transactions. They had a couple of transactions in a short period of time. The amounts spent are quite regular as well.\n",
    "\n",
    "#### The biggest total_spent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf0e17",
   "metadata": {},
   "source": [
    "```Showing top 10 of total_spent descending\n",
    "customer_id  transactions  avg_spent  max_spent  min_spent  total_spent    first_transaction     last_transaction  purchase_interval\n",
    "       1725            36     105.74     210.19      12.36      3806.59  2020-01-29 06:53:00  2020-12-06 07:54:00                8.9\n",
    "       1655            34     106.63     175.60      39.19      3625.30  2020-01-04 16:48:00  2020-11-02 03:57:00                9.2\n",
    "       3407            33     106.00     215.77      46.34      3498.12  2020-02-10 19:49:00  2020-12-01 11:36:00                9.2\n",
    "       4201            30     114.62     186.04      36.94      3438.49  2020-01-20 01:14:00  2020-12-12 12:00:00               11.3\n",
    "       3893            32     103.84     199.95       7.11      3322.99  2020-01-03 05:56:00  2020-12-09 11:33:00               11.0\n",
    "       2022            31     104.70     191.65      13.16      3245.72  2020-01-10 00:20:00  2020-11-27 08:59:00               10.7\n",
    "        676            29     110.72     174.90      13.18      3210.90  2020-01-02 02:36:00  2020-12-06 16:14:00               12.1\n",
    "         72            29     109.64     195.97       3.81      3179.51  2020-01-02 18:46:00  2020-11-28 15:37:00               11.8\n",
    "        107            32      99.36     199.06      14.87      3179.47  2020-01-06 00:52:00  2020-11-28 06:06:00               10.6\n",
    "       2934            32      98.58     219.59     -23.16      3154.43  2020-01-03 08:33:00  2020-11-22 11:59:00               10.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624ea63",
   "metadata": {},
   "source": [
    "Their intervals vary between 1 and 2 weeks, but they all have almost 30 transactions. The average spending amounts are marginally even lower than the visitors from the first table. They have transactions from the beginning of the time scope until the end. This table is naturally very similar to the amount of transactions:\n",
    "\n",
    "#### The most transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645661a2",
   "metadata": {},
   "source": [
    "```Showing top 10 of transactions descending\n",
    "customer_id  transactions  avg_spent  max_spent  min_spent  total_spent    first_transaction     last_transaction  purchase_interval\n",
    "       1725            36     105.74     210.19      12.36      3806.59  2020-01-29 06:53:00  2020-12-06 07:54:00                8.9\n",
    "       1655            34     106.63     175.60      39.19      3625.30  2020-01-04 16:48:00  2020-11-02 03:57:00                9.2\n",
    "       3407            33     106.00     215.77      46.34      3498.12  2020-02-10 19:49:00  2020-12-01 11:36:00                9.2\n",
    "       3893            32     103.84     199.95       7.11      3322.99  2020-01-03 05:56:00  2020-12-09 11:33:00               11.0\n",
    "        107            32      99.36     199.06      14.87      3179.47  2020-01-06 00:52:00  2020-11-28 06:06:00               10.6\n",
    "       2934            32      98.58     219.59     -23.16      3154.43  2020-01-03 08:33:00  2020-11-22 11:59:00               10.5\n",
    "       3576            31     100.56     187.77      22.48      3117.44  2020-01-02 03:25:00  2020-12-03 15:40:00               11.2\n",
    "       2022            31     104.70     191.65      13.16      3245.72  2020-01-10 00:20:00  2020-11-27 08:59:00               10.7\n",
    "       2355            30     102.22     192.98      35.53      3066.74  2020-01-06 09:04:00  2020-12-11 14:19:00               11.7\n",
    "       4295            30     104.32     175.42      23.48      3129.68  2020-01-20 01:51:00  2020-12-01 09:29:00               10.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f256619",
   "metadata": {},
   "source": [
    "Identical top 3 and the rest consist almost of the same customers too. The biggest one time spenders are bringing in a decent amount of cash flow as well:\n",
    "\n",
    "#### The biggest max_spent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355dc59",
   "metadata": {},
   "source": [
    "```Showing top 10 of max_spent descending\n",
    "customer_id  transactions  avg_spent  max_spent  min_spent  total_spent    first_transaction     last_transaction  purchase_interval\n",
    "       1877            19     115.55     263.11      38.07      2195.37  2020-01-04 09:35:00  2020-12-06 18:04:00               18.7\n",
    "       3195            21      98.78     253.31      35.79      2074.42  2020-01-29 00:51:00  2020-12-07 12:44:00               15.7\n",
    "        489            18     111.34     252.85      56.55      2004.13  2020-01-05 12:43:00  2020-11-27 05:32:00               19.2\n",
    "       3374            13     113.94     251.14      36.38      1481.26  2020-01-01 12:20:00  2020-11-30 14:27:00               27.8\n",
    "       2933            12     105.69     250.88      43.32      1268.22  2020-01-30 20:41:00  2020-11-23 03:24:00               27.0\n",
    "       4754            20     106.88     249.66      40.50      2137.55  2020-01-05 16:21:00  2020-11-06 06:36:00               16.1\n",
    "       2564            17     122.73     245.24      72.47      2086.42  2020-01-16 05:50:00  2020-11-22 18:25:00               19.5\n",
    "       3907            15     129.22     239.18      35.82      1938.36  2020-05-04 17:26:00  2020-11-10 22:48:00               13.6\n",
    "        766            18     104.01     236.87      39.03      1872.22  2020-03-22 11:18:00  2020-12-11 23:04:00               15.6\n",
    "       1517            25     102.82     236.75      20.41      2570.44  2020-02-20 20:00:00  2020-11-13 02:38:00               11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5ab93",
   "metadata": {},
   "source": [
    "And finally, the churning risk:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9d7c0",
   "metadata": {},
   "source": [
    "#### The longest since their last transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff70bc4",
   "metadata": {},
   "source": [
    "```Showing top 10 of days_since_last_transaction descending\n",
    "customer_id  transactions  total_spent  purchase_interval  days_since_last_transaction\n",
    "       2502             9       894.24               21.6                        185.0\n",
    "       2214             9      1135.11               26.4                        153.0\n",
    "       1080            12      1163.22               18.2                        151.0\n",
    "       1041            12      1031.66               18.4                        146.0\n",
    "       1147            14      1237.58               16.2                        144.0\n",
    "        172            20      1890.06               12.2                        125.0\n",
    "       4906            19      1549.62               13.5                        121.0\n",
    "       2905            14      1299.31               18.9                        120.0\n",
    "       4911            15      1653.80               14.9                        117.0\n",
    "       1881            11      1134.18               19.6                        111.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464a0be",
   "metadata": {},
   "source": [
    "I suppose one should prioritize these customers by their `total_spent` amount to keep the most valuable customers, if prioritization is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21c6a7",
   "metadata": {},
   "source": [
    "### Downstream use\n",
    "\n",
    "Both dataframes are saved in their entirety as `customer_features.csv` and `churning_risks.csv` in the `./data` directory. From there they could be easily fed to a BI tool for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d1648",
   "metadata": {},
   "source": [
    "## Part 3: LLM pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b89a450",
   "metadata": {},
   "source": [
    "Given the minimal document set (11 lines total), I decided to mock the RAG pipeline to demonstrate the architecture rather than implement actual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe54dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "from utils.file_reader import get_filenames\n",
    "\n",
    "\n",
    "def vectorize(query: str = \"This is a query.\") -> list[float]:\n",
    "  embedding = []\n",
    "  logger.info(f\"Vectorizing query: {query}\")\n",
    "  files = get_filenames()\n",
    "  for _ in files:\n",
    "    embedding.append(random())\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb100c7",
   "metadata": {},
   "source": [
    "So I start by \"vectorizing\" the query. The `vectorize` function returns a list of floats. The list has the same amount of elements, as there are documents in the documents folder. The values are random values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e768614",
   "metadata": {},
   "source": [
    "With the embedding, we can retrieve with \"vector search\" or \"cosine similarity\". Instead of implementing that, or even vectorizing the source material, decided to treat the random values of the vector as the cosine similarity score for the respective documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b07068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reader import read_doc\n",
    "\n",
    "def retrieve(vector: list[float], top_k: int, threshold: float) -> dict:\n",
    " \n",
    "  context = {}\n",
    "  filenames = get_filenames()\n",
    "  scores = {}\n",
    "  # Give each filename their mock cosine similarity value\n",
    "  for idx, filename in enumerate(filenames):\n",
    "    scores[filename] = vector[idx]\n",
    "  # Sort the filenames by said values\n",
    "  top_results = sorted(scores, key=scores.get, reverse=True)\n",
    "  # Get top-k results that are over the threshold value \n",
    "  for result in top_results[:top_k]:\n",
    "    if scores[result] > threshold:\n",
    "      context[result] = read_doc(f\"./data/documents/{result}\")\n",
    "  return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e22a30",
   "metadata": {},
   "source": [
    "With the context (if found), I can produce the \"response\", which I also mocked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(context) -> str:\n",
    "  response = \"Hi! I'm your happy assistant!\\n\"\n",
    "  if context:\n",
    "    response += \"I'm basing my helpful response to the following contents:\\n\"\n",
    "    for item in context:\n",
    "      response +=f\"\\n{item}\\n{context[item]}\\n\"\n",
    "  else:\n",
    "    response += \"This time we did not find what you were looking for.\"\n",
    "  \n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9199b",
   "metadata": {},
   "source": [
    "## Part 4: Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc7cde",
   "metadata": {},
   "source": [
    "I built a simple API with emphasis on the simple. It has 2 endpoints in the end. One for \"asking questions about the documents\" and the other for uploading more documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from utils.filename_validation import validate_filename\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "def ask(query: str, top_k: int, threshold: float):\n",
    "  embedding = vectorize(query)\n",
    "  context = retrieve(embedding, top_k, threshold)\n",
    "  response = generate_response(context)\n",
    "  return {\"query\": query, \"response\": response}\n",
    "\n",
    "@app.post(\"/upload\")\n",
    "def upload(filename: str, content: str):\n",
    "  validated_filename = validate_filename(filename)\n",
    "  if type(validated_filename) == str:\n",
    "    with open(f\"./data/documents/{validated_filename}\", \"w\") as file:\n",
    "      file.write(content)\n",
    "    return { \"status\": 200 }\n",
    "  else:\n",
    "    return { \"status\": 403, \"details\": validated_filename[\"reason\"] }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97780fed",
   "metadata": {},
   "source": [
    "I added a basic validation for filenames, so only .txt's can get through and the user can't overwrite existing files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad39898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reader import get_filenames\n",
    "\n",
    "ValidationObject = dict[str, str | bool]\n",
    "\n",
    "def validate_filename(filename) -> str | ValidationObject:\n",
    "  if filename.endswith('.txt'):\n",
    "    if '.' in filename[:-4]:\n",
    "      return { \"success\": False, \"reason\": \"Invalid filename\" }\n",
    "  else:\n",
    "    if '.' in filename:\n",
    "      return { \"success\": False, \"reason\": \"Invalid filename\" }\n",
    "    filename += '.txt'\n",
    "\n",
    "  if filename in get_filenames():\n",
    "    return { \"success\": False, \"reason\": \"Duplicate filename\" }\n",
    "  \n",
    "  return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d876f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This project demonstrates a complete data pipeline:\n",
    "\n",
    "1. **ETL**: Extracted CSVs, transformed types, loaded to SQLite\n",
    "2. **Data Quality**: Identified ~85k problematic transactions, 2k duplicates, created clean view\n",
    "3. **Feature Engineering**: Built customer metrics (spend, frequency, churn risk)\n",
    "4. **LLM Pipeline**: Mock RAG demonstrating retrieve → augment → generate flow\n",
    "5. **API**: FastAPI service with /ask and /upload endpoints\n",
    "\n",
    "**Key insight**: Revenue correlates with transaction frequency, not transaction size. Customer retention is more valuable than pursuing large one-time sales.\n",
    "\n",
    "**Output**: Clean feature tables exported to CSV, ready for BI tools or further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accenture-take-home",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
