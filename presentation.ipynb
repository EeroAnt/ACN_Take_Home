{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7925620a",
   "metadata": {},
   "source": [
    "# Take-home Assignment by Eero Antikainen\n",
    "\n",
    "To start with, we can just run the script as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58af8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023348e",
   "metadata": {},
   "source": [
    "## Part 1: ETL\n",
    "\n",
    "At first I glanced at the csv's to see what's in them.\n",
    "\n",
    "customers.csv looked mostly ok, transactions.csv had a good amount of missing values.\n",
    "\n",
    "For this amount of data and the complexity of processes I planned to do, I decided SQLite is more than fine.\n",
    "\n",
    "To begin with, I thought that I'd load the data pretty much as is to the database for easy querying.\n",
    "\n",
    "So after setting up the database, the only transformations I decided to do here is to turn missing values into None instead of empty strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b837af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_customers(raw_data: list[tuple]) -> list[tuple]:\n",
    "  return [\n",
    "    (\n",
    "      int(customer_id) if customer_id else None,\n",
    "      country if country else None,\n",
    "      signup_date if signup_date else None,\n",
    "      email if email else None\n",
    "    )\n",
    "    for customer_id, country, signup_date, email in raw_data\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afebf290",
   "metadata": {},
   "source": [
    "This transformation allowed me to insert the missing values as NULL easily.\n",
    "\n",
    "## Part 2: Feature engineering\n",
    "\n",
    "I proceeded to do some anomaly checks to understand the problems of the data better.\n",
    "\n",
    "Results displayed from the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b418e7e",
   "metadata": {},
   "source": [
    "```2026-02-05 21:22:17,924 - INFO - Found 30 NULL values in transactions table column customer_id\n",
    "2026-02-05 21:22:17,924 - INFO - Total value: 1410.28 EUR\n",
    "2026-02-05 21:22:17,925 - INFO - Found 24644 NULL values in transactions table column currency\n",
    "2026-02-05 21:22:17,925 - INFO - Found 20220 NULL values in transactions table column category\n",
    "2026-02-05 21:22:17,929 - INFO - Total value: 887961.98 EUR\n",
    "2026-02-05 21:22:18,014 - INFO - 85435 transactions found where timestamp precedes users signup\n",
    "2026-02-05 21:22:18,033 - INFO - Total value: 3717602.34 EUR\n",
    "2026-02-05 21:22:18,083 - INFO - Found 2000 duplicated transaction ids\n",
    "2026-02-05 21:22:18,084 - INFO - Getting deeper comparison\n",
    "2026-02-05 21:22:18,213 - INFO - Found 2000 duplicated transactions\n",
    "2026-02-05 21:22:18,214 - INFO - All the duplicates are exact matches and can be filtered via 'GROUP BY transaction_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc00c8f",
   "metadata": {},
   "source": [
    "The first part was straightforward query of NULL values for each column in both tables and logging results, when found.\n",
    "\n",
    "I also logged the total values transferred to Euros (except for NULL values in currency column)\n",
    "\n",
    "Turns out the customers data was fine and transactions had more inconsistencies.\n",
    "\n",
    "I decided to assume that category was not a required information, but others I would flag for closer inspection.\n",
    "\n",
    "Both tables had time related column in them so I wanted to check for inconsistencies there.\n",
    "\n",
    "And I found 3.7 million Eur worth of transactions by users before their signup. More flags.\n",
    "\n",
    "The last anomaly check was found by accident when I was looking at `average_days_between_transactions` (now `purchase_interval`) data and at the top of the list there was a user with 2 transactions, 0 days in between, and identical sum. This inspired me to check possible duplicate id's, after which I did a deeper comparison. I suppose one could've gone just with the deeper comparison straight away, but I thought there might have been transactions that share only the transaction id because of a race condition or something similar.\n",
    "\n",
    "With all this insight, I cleaned the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "from utils.logging_config import logger\n",
    "\n",
    "\n",
    "def clean_transactions(conn: sqlite3.Connection) -> None:\n",
    "  logger.info(\"Cleaning the dataset after anomaly analysis\")\n",
    "  cur = conn.cursor()\n",
    "  cur.execute(\n",
    "    \"\"\"\n",
    "      CREATE VIEW clean_transactions AS\n",
    "      SELECT\n",
    "        transaction_id,\n",
    "        transactions.customer_id customer_id,\n",
    "        amount,\n",
    "        currency,\n",
    "        timestamp,\n",
    "        category\n",
    "      FROM transactions\n",
    "      INNER JOIN customers\n",
    "      ON transactions.customer_id = customers.customer_id\n",
    "      WHERE currency IS NOT NULL\n",
    "        AND transactions.customer_id IS NOT NULL\n",
    "        AND transactions.timestamp >= customers.signup_date\n",
    "      GROUP BY transaction_id\n",
    "    \"\"\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb18a92",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "During the process I rotated the data a handful of ways to find something interesting. Here are some insights from the views I ended up in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936fabe",
   "metadata": {},
   "source": [
    "```Showing top 10 of purchase_interval ascending\n",
    "customer_id  transactions  avg_spent  max_spent  min_spent  total_spent    first_transaction     last_transaction  purchase_interval\n",
    "       1714             2     121.14     179.30      62.97       242.27  2020-11-18 23:04:00  2020-11-19 13:32:00                0.6\n",
    "       3980             2      97.74     118.38      77.09       195.47  2020-12-09 04:53:00  2020-12-10 05:43:00                1.0\n",
    "       2327             4     130.17     184.07      56.93       520.66  2020-11-10 08:58:00  2020-11-14 01:39:00                1.2\n",
    "       4391             3     119.97     183.59      87.04       359.92  2020-12-08 07:19:00  2020-12-11 21:24:00                1.8\n",
    "       3014             2     101.25     109.01      93.48       202.49  2020-11-09 15:09:00  2020-11-11 11:24:00                1.8\n",
    "        136             2     115.91     130.87     100.95       231.82  2020-11-05 15:54:00  2020-11-08 04:02:00                2.5\n",
    "       3637             2     110.53     113.18     107.87       221.05  2020-10-20 10:16:00  2020-10-23 12:06:00                3.1\n",
    "        117             2     169.10     187.12     151.07       338.19  2020-11-26 03:28:00  2020-11-29 13:05:00                3.4\n",
    "       3587             3     126.94     204.91      77.08       380.83  2020-09-16 13:25:00  2020-09-23 20:25:00                3.6\n",
    "       4280             5     139.75     197.68     104.88       698.75  2020-10-18 08:45:00  2020-11-02 17:48:00                3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142534f3",
   "metadata": {},
   "source": [
    "Here we see the lowest puchase intervals, the highest transaction freqencies. It seems that none of them have many transactions. They had a couple of transactions in a short period of time. The amounts spent are quite regular as well.\n",
    "\n",
    "Now the big spenders:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf0e17",
   "metadata": {},
   "source": [
    "```Showing top 10 of total_spent descending\n",
    "customer_id  transactions  avg_spent  max_spent  min_spent  total_spent    first_transaction     last_transaction  purchase_interval\n",
    "       1725            36     105.74     210.19      12.36      3806.59  2020-01-29 06:53:00  2020-12-06 07:54:00                8.9\n",
    "       1655            34     106.63     175.60      39.19      3625.30  2020-01-04 16:48:00  2020-11-02 03:57:00                9.2\n",
    "       3407            33     106.00     215.77      46.34      3498.12  2020-02-10 19:49:00  2020-12-01 11:36:00                9.2\n",
    "       4201            30     114.62     186.04      36.94      3438.49  2020-01-20 01:14:00  2020-12-12 12:00:00               11.3\n",
    "       3893            32     103.84     199.95       7.11      3322.99  2020-01-03 05:56:00  2020-12-09 11:33:00               11.0\n",
    "       2022            31     104.70     191.65      13.16      3245.72  2020-01-10 00:20:00  2020-11-27 08:59:00               10.7\n",
    "        676            29     110.72     174.90      13.18      3210.90  2020-01-02 02:36:00  2020-12-06 16:14:00               12.1\n",
    "         72            29     109.64     195.97       3.81      3179.51  2020-01-02 18:46:00  2020-11-28 15:37:00               11.8\n",
    "        107            32      99.36     199.06      14.87      3179.47  2020-01-06 00:52:00  2020-11-28 06:06:00               10.6\n",
    "       2934            32      98.58     219.59     -23.16      3154.43  2020-01-03 08:33:00  2020-11-22 11:59:00               10.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624ea63",
   "metadata": {},
   "source": [
    "Their intervals vary between 1 and 2 weeks, but they all have almost 30 transactions. The average spending amounts are marginally even lower than the visitors from the first table. They have transactions from the beginning of the time scope until the end. This table is naturally very similar to the amount of transactions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645661a2",
   "metadata": {},
   "source": [
    "```Showing top 10 of transactions descending\n",
    "customer_id  transactions  avg_spent  max_spent  min_spent  total_spent    first_transaction     last_transaction  purchase_interval\n",
    "       1725            36     105.74     210.19      12.36      3806.59  2020-01-29 06:53:00  2020-12-06 07:54:00                8.9\n",
    "       1655            34     106.63     175.60      39.19      3625.30  2020-01-04 16:48:00  2020-11-02 03:57:00                9.2\n",
    "       3407            33     106.00     215.77      46.34      3498.12  2020-02-10 19:49:00  2020-12-01 11:36:00                9.2\n",
    "       3893            32     103.84     199.95       7.11      3322.99  2020-01-03 05:56:00  2020-12-09 11:33:00               11.0\n",
    "        107            32      99.36     199.06      14.87      3179.47  2020-01-06 00:52:00  2020-11-28 06:06:00               10.6\n",
    "       2934            32      98.58     219.59     -23.16      3154.43  2020-01-03 08:33:00  2020-11-22 11:59:00               10.5\n",
    "       3576            31     100.56     187.77      22.48      3117.44  2020-01-02 03:25:00  2020-12-03 15:40:00               11.2\n",
    "       2022            31     104.70     191.65      13.16      3245.72  2020-01-10 00:20:00  2020-11-27 08:59:00               10.7\n",
    "       2355            30     102.22     192.98      35.53      3066.74  2020-01-06 09:04:00  2020-12-11 14:19:00               11.7\n",
    "       4295            30     104.32     175.42      23.48      3129.68  2020-01-20 01:51:00  2020-12-01 09:29:00               10.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f256619",
   "metadata": {},
   "source": [
    "Identical top 3 and the rest consist almost of the same customers too. The biggest one time spenders are bringing in a decent amount of cash flow as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355dc59",
   "metadata": {},
   "source": [
    "```Showing top 10 of max_spent descending\n",
    "customer_id  transactions  avg_spent  max_spent  min_spent  total_spent    first_transaction     last_transaction  purchase_interval\n",
    "       1877            19     115.55     263.11      38.07      2195.37  2020-01-04 09:35:00  2020-12-06 18:04:00               18.7\n",
    "       3195            21      98.78     253.31      35.79      2074.42  2020-01-29 00:51:00  2020-12-07 12:44:00               15.7\n",
    "        489            18     111.34     252.85      56.55      2004.13  2020-01-05 12:43:00  2020-11-27 05:32:00               19.2\n",
    "       3374            13     113.94     251.14      36.38      1481.26  2020-01-01 12:20:00  2020-11-30 14:27:00               27.8\n",
    "       2933            12     105.69     250.88      43.32      1268.22  2020-01-30 20:41:00  2020-11-23 03:24:00               27.0\n",
    "       4754            20     106.88     249.66      40.50      2137.55  2020-01-05 16:21:00  2020-11-06 06:36:00               16.1\n",
    "       2564            17     122.73     245.24      72.47      2086.42  2020-01-16 05:50:00  2020-11-22 18:25:00               19.5\n",
    "       3907            15     129.22     239.18      35.82      1938.36  2020-05-04 17:26:00  2020-11-10 22:48:00               13.6\n",
    "        766            18     104.01     236.87      39.03      1872.22  2020-03-22 11:18:00  2020-12-11 23:04:00               15.6\n",
    "       1517            25     102.82     236.75      20.41      2570.44  2020-02-20 20:00:00  2020-11-13 02:38:00               11.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accenture-take-home",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
